---
title: Gradient Descent Algorithms from Scratch
published: 2025-08-03
description: Implementing many common gradient descent algorithms
tags: [julia, optimization]
category: Guides
draft: true
---

One of the simplest classes of minimization algorithms that sees common use are gradient descent algorithms. These algorithms have progressed since originally developed by (TODO). While Julia packages such as [Optimizers.jl](https://fluxml.ai/Optimisers.jl/stable/) have a large collection of various gradient descent algorithms, there is a lack of easily hackable Julia code to work off from. This allows further fine-tuning for your algorithm at the expense of generality whilst minimizing overhead from the rest of the library. We will be covering the following gradient descent algorithms:

- Gradient Descent;
- Stochastic Gradient Descent;
- Gradient Descent with Nesterov Momentum;
- Adam;
- Projected Gradient Descent;
- Distributed Gradient Descent;

## Main Content Section 1



## Conclusion

Summarize your main points and reinforce the key takeaways. End with a call-to-action that encourages reader engagement, whether that's trying something new, sharing their thoughts, or exploring related topics.

---

## Additional Resources

- [Resource 1](URL) - Brief description
- [Resource 2](URL) - Brief description
- [Resource 3](URL) - Brief description

---

ðŸ’¡ Found this helpful? If you enjoyed this article and want to see more content like this, consider buying me a coffee or sharing it with someone who might find it useful. Your support helps keep the content coming and a PhD student sufficiently caffeinated! âœ¨

